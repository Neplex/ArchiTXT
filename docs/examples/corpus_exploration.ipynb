{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "Exploring a Textual Corpus with ArchiTXT\n",
    "========================================\n",
    "\n",
    "This tutorial provides a **step-by-step guide** on how to use **ArchiTXT** to efficiently process and analyze textual corpora.\n",
    "\n",
    "ArchiTXT allows loading a corpus as a set of syntax trees, where each tree is enriched by incorporating named entities.\n",
    "These enriched trees form a **forest**, which can then be automatically structured into a valid **database instance** for further analysis.\n",
    "\n",
    "By following this tutorial, you'll learn how to:\n",
    "- Load a corpus\n",
    "- Parse textual data with **Berkeley Neural Parser (Benepar)**\n",
    "- Extract structured data using **ArchiTXT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import itables\n",
    "\n",
    "itables.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Downloading the MACCROBAT Corpus\n",
    "The **MACCROBAT** corpus is a collection of **200 annotated medical documents**, specifically **clinical case reports**, extracted from **PubMed Central**.\n",
    "The annotations focus on key medical concepts such as **diseases, treatments, medications, and symptoms**, making it a valuable resource for biomedical text analysis.\n",
    "\n",
    "The **MACCROBAT** corpus is available for download at [Figshare](https://figshare.com/articles/dataset/MACCROBAT2018/9764942) or on [kaggle](https://www.kaggle.com/datasets/okolojeremiah/maccrobat).\n",
    "\n",
    "Let's download the corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "urllib.request.urlretrieve(\n",
    "    'https://www.kaggle.com/api/v1/datasets/download/okolojeremiah/maccrobat',\n",
    "    filename='MACCROBAT.zip',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Installing and Configuring NLP Models\n",
    "\n",
    "ArchiTXT can parse the sentences using either **Benepar** with SpaCy or a **CoreNLP** server.\n",
    "In this tutorial, we will use the **SpaCy parser** with the default model, but you can use any models like one from **SciSpaCy**, a collection of models designed for biomedical text processing by **AllenAI**.\n",
    "\n",
    "To download the SciSpaCy model, do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "!spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "We also need to download the Benepar model for English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "import benepar\n",
    "\n",
    "benepar.download('benepar_en3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Parsing the Corpus with ArchiTXT\n",
    "\n",
    "Before processing the corpus, we need to configure the **BeneparParser**, specifying which SpaCy model to use for each language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "from architxt.nlp.parser.benepar import BeneparParser\n",
    "\n",
    "# Initialize the parser\n",
    "parser = BeneparParser(\n",
    "    spacy_models={\n",
    "        'English': 'en_core_web_sm',\n",
    "    }\n",
    ")\n",
    "\n",
    "# Suppress warnings for unsupported annotations\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "Named Entity Resolution (NER) helps to standardize the named entities and to build a database instance.\n",
    "To enable NER, we need to provide the knowledge base to use.\n",
    "For this tutorial, we will use the **UMLS (Unified Medical Language System)** resolver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from architxt.nlp.entity_resolver import ScispacyResolver\n",
    "\n",
    "resolver = ScispacyResolver(kb_name='umls')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "Let's parse a sample of the corpus. To verify that everything is functioning as expected, we will inspect the largest enriched tree using the :py:meth:`~architxt.tree.Tree.pretty_print` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from architxt.nlp import raw_load_corpus\n",
    "\n",
    "forest = [\n",
    "    tree\n",
    "    async for tree in raw_load_corpus(\n",
    "        ['MACCROBAT.zip'],\n",
    "        ['English'],\n",
    "        cache=False,\n",
    "        parser=parser,\n",
    "        resolver=resolver,\n",
    "        sample=600,\n",
    "        entities_filter={\n",
    "            'OTHER_ENTITY',\n",
    "            'OTHER_EVENT',\n",
    "            'COREFERENCE',\n",
    "        },\n",
    "        entities_mapping={\n",
    "            'QUANTITATIVE_CONCEPT': 'VALUE',\n",
    "            'QUALITATIVE_CONCEPT': 'VALUE',\n",
    "            'LAB_VALUE': 'VALUE',\n",
    "            'THERAPEUTIC_PROCEDURE': 'TREATMENT',\n",
    "            'MEDICATION': 'TREATMENT',\n",
    "            'OUTCOME': 'SIGN_SYMPTOM',\n",
    "        },\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the highest tree\n",
    "max(forest, key=lambda tree: tree.height).pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "Let's see the repartition of the entities inside this sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "entity_count = Counter(entity.label.name for tree in forest for entity in tree.entities())\n",
    "\n",
    "sorted_entities = sorted(entity_count.items(), key=lambda x: x[1], reverse=True)\n",
    "entities = [label for label, count in sorted_entities]\n",
    "counts = [count for label, count in sorted_entities]\n",
    "\n",
    "fig = px.histogram(y=entities, x=counts, orientation='h')\n",
    "fig.update_layout(xaxis_title='Count', yaxis_title='Entities')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "source": [
    " **ArchiTXT** can then automatically structure parsed text into a **database-friendly format**.\n",
    " Let's start with a simple rewrite!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "from architxt.simplification.simple_rewrite import simple_rewrite\n",
    "\n",
    "forest_copy = deepcopy(forest)\n",
    "simple_rewrite(forest_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the highest tree\n",
    "max(forest_copy, key=lambda tree: tree.height).pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "Now that we have a structured instance, we can extract its schema.\n",
    "The schema provides a **formal representation** of the extracted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [],
   "source": [
    "from architxt.schema import Schema\n",
    "\n",
    "schema = Schema.from_forest(forest_copy, keep_unlabelled=False)\n",
    "print(schema.as_cfg())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "We've successfully built a basic database schema from our corpus, but there's significant potential for improvement.\n",
    "Let's explore how we can enhance it using the **ArchiTXT** simplification algorithm!\n",
    "\n",
    "First, let's visualize the repartition of equivalent classes inside the forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "from architxt.similarity import equiv_cluster\n",
    "\n",
    "clusters = equiv_cluster(forest, tau=0.8, decay=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "Let's visualize the clustering result as a bar chart to better understand the distribution of groups across equivalent classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "clusters_names = sorted(filter(lambda klass: len(clusters[klass]) >= 5, clusters.keys()))\n",
    "fig = px.bar(y=clusters_names, x=[len(clusters[klass]) for klass in clusters_names], orientation='h')\n",
    "\n",
    "fig.update_layout(xaxis_title='Count', yaxis_title='Equivalent Class')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "It's now time to use **ArchiTXT** to automatically structure the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "from architxt.simplification.tree_rewriting import rewrite\n",
    "\n",
    "rewrite(forest, epoch=30, min_support=5, tau=0.8, decay=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the highest tree\n",
    "max(forest, key=lambda tree: tree.height).pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "We now have a more granular structure. Let's take a closer look at the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = Schema.from_forest(forest, keep_unlabelled=False)\n",
    "print(schema.as_cfg())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "The schema is now much smaller, and the groups are more meaningful.\n",
    "\n",
    "But not all extracted trees provide valuable insights, so we could filter the structured instance to keep only the **valid trees** using `schema.extract_valid_trees(new_forest)`.\n",
    "Let's explore the different **semantic groups**.\n",
    "Groups represent common patterns across the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_datasets = schema.extract_datasets(forest)\n",
    "group, dataset = max(all_datasets.items(), key=lambda x: len(x[1]))\n",
    "\n",
    "print(f'Group: {group}')\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "## Export as a property graph\n",
    "\n",
    "Now that we've integrated our two databases, we can export the result as a **property graph**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "We use `testcontainers` to spin up a **disposable Neo4j instance** for safe experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from testcontainers.neo4j import Neo4jContainer\n",
    "\n",
    "neo4j = Neo4jContainer('neo4j:5')\n",
    "neo4j.start()\n",
    "uri = neo4j.get_connection_url()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "ArchiTXT makes it easy to export structured data like a tree or forest directly into a property graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "from architxt.database.export import export_cypher\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "driver = GraphDatabase.driver(uri, auth=('neo4j', 'password'))\n",
    "\n",
    "with driver.session() as session:\n",
    "    export_cypher(forest, session=session)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "Let's explore the generated graph database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yfiles_jupyter_graphs_for_neo4j import Neo4jGraphWidget\n",
    "\n",
    "g = Neo4jGraphWidget(driver)\n",
    "g.show_cypher(\"\"\"\n",
    "MATCH (n)\n",
    "OPTIONAL MATCH path = (n)-[*..4]-()\n",
    "RETURN n, path\n",
    "LIMIT 50\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
