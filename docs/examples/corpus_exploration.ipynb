{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b02239d8fef12638",
      "metadata": {},
      "source": [
        "Exploring a Textual Corpus with ArchiTXT\n",
        "========================================\n",
        "\n",
        "This tutorial provides a **step-by-step guide** on how to use **ArchiTXT** to efficiently process and analyze textual corpora.\n",
        "\n",
        "ArchiTXT allows loading a corpus as a set of syntax trees, where each tree is enriched by incorporating named entities.\n",
        "These enriched trees form a **forest**, which can then be automatically structured into a valid **database instance** for further analysis.\n",
        "\n",
        "By following this tutorial, you'll learn how to:\n",
        "- Load a corpus\n",
        "- Parse textual data with **Berkeley Neural Parser (Benepar)**\n",
        "- Extract structured data using **ArchiTXT**"
      ]
    },
    {
      "cell_type": "code",
      "id": "31802aa658488f0f",
      "metadata": {
        "tags": [
          "remove-cell"
        ]
      },
      "source": [
        "import itables\n",
        "\n",
        "itables.init_notebook_mode(connected=True)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "115e771cc1456e16",
      "metadata": {},
      "source": [
        "## Downloading the MACCROBAT Corpus\n",
        "The **MACCROBAT** corpus is a collection of **200 annotated medical documents**, specifically **clinical case reports**, extracted from **PubMed Central**.\n",
        "The annotations focus on key medical concepts such as **diseases, treatments, medications, and symptoms**, making it a valuable resource for biomedical text analysis.\n",
        "\n",
        "The **MACCROBAT** corpus is available for download at [Figshare](https://figshare.com/articles/dataset/MACCROBAT2018/9764942).\n",
        "\n",
        "Let's download the corpora."
      ]
    },
    {
      "cell_type": "code",
      "id": "9bb17adb4e73b73",
      "metadata": {},
      "source": [
        "import io\n",
        "import urllib.request\n",
        "import zipfile\n",
        "\n",
        "with urllib.request.urlopen('https://figshare.com/ndownloader/articles/9764942/versions/2') as response:\n",
        "    archive_file = io.BytesIO(response.read())\n",
        "\n",
        "with zipfile.ZipFile(archive_file) as archive:\n",
        "    archive.extract('MACCROBAT2020.zip')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "f53f334f1db883be",
      "metadata": {},
      "source": [
        "## Installing and Configuring NLP Models\n",
        "\n",
        "ArchiTXT can parse the sentences using either **Benepar** with SpaCy or a **CoreNLP** server.\n",
        "In this tutorial, we will use the **SpaCy parser** with the default model, but you can use any models like one from **SciSpaCy**, a collection of models designed for biomedical text processing by **AllenAI**.\n",
        "\n",
        "To download the SciSpaCy model, do:"
      ]
    },
    {
      "cell_type": "code",
      "id": "f3f45627cd6a6589",
      "metadata": {
        "tags": [
          "remove-output"
        ]
      },
      "source": [
        "!spacy download en_core_web_sm"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "34644a1af9adac4d",
      "metadata": {},
      "source": "We also need to download the Benepar model for English"
    },
    {
      "cell_type": "code",
      "id": "97d57983410cd61",
      "metadata": {
        "tags": [
          "remove-output"
        ]
      },
      "source": [
        "import benepar\n",
        "\n",
        "benepar.download('benepar_en3')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "964d0fa93b704e92",
      "metadata": {},
      "source": [
        "## Parsing the Corpus with ArchiTXT\n",
        "\n",
        "Before processing the corpus, we need to configure the **BeneparParser**, specifying which SpaCy model to use for each language."
      ]
    },
    {
      "cell_type": "code",
      "id": "e14ee4fbe193af94",
      "metadata": {
        "tags": [
          "remove-output"
        ]
      },
      "source": [
        "import warnings\n",
        "\n",
        "from architxt.nlp.parser.benepar import BeneparParser\n",
        "\n",
        "# Initialize the parser\n",
        "parser = BeneparParser(\n",
        "    spacy_models={\n",
        "        'English': 'en_core_web_sm',\n",
        "    }\n",
        ")\n",
        "\n",
        "# Suppress warnings for unsupported annotations\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "d5e84953743102d3",
      "metadata": {},
      "source": [
        "Named Entity Resolution (NER) helps to standardize the named entities and to build a database instance.\n",
        "To enable NER, we need to provide the knowledge base to use.\n",
        "For this tutorial, we will use the **UMLS (Unified Medical Language System)** resolver.\n",
        "\n",
        "Let's parse a sample of the corpus. To verify that everything is functioning as expected, we will inspect the largest enriched tree using the :py:meth:`~architxt.tree.Tree.pretty_print` method."
      ]
    },
    {
      "metadata": {},
      "cell_type": "code",
      "source": [
        "from architxt.nlp import raw_load_corpus\n",
        "\n",
        "forest = [\n",
        "    tree\n",
        "    async for tree in raw_load_corpus(\n",
        "        ['MACCROBAT2020.zip'],\n",
        "        ['English'],\n",
        "        parser=parser,\n",
        "        resolver_name='umls',\n",
        "        sample=400,\n",
        "    )\n",
        "]\n",
        "\n",
        "# Look at the highest tree\n",
        "max(forest, key=lambda tree: tree.height).pretty_print()"
      ],
      "id": "52153bd81abab8d3",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        " **ArchiTXT** can then automatically structure parsed text into a **database-friendly format**.\n",
        " Let's start with a simple rewrite!"
      ],
      "id": "1ca7e8642db92b0d"
    },
    {
      "metadata": {
        "tags": [
          "remove-output"
        ]
      },
      "cell_type": "code",
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "from architxt.simplification.simple_rewrite import simple_rewrite\n",
        "\n",
        "forest_copy = deepcopy(forest)\n",
        "simple_rewrite(forest_copy)"
      ],
      "id": "8bb39d9e2581dd0c",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "9f27bab9088de6ec",
      "metadata": {},
      "source": [
        "# Look at the highest tree\n",
        "max(forest_copy, key=lambda tree: tree.height).pretty_print()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "2097e213eedb2081",
      "metadata": {},
      "source": [
        "Now that we have a structured instance, we can extract its schema.\n",
        "The schema provides a **formal representation** of the extracted data."
      ]
    },
    {
      "cell_type": "code",
      "id": "d002e40e0ab4287d",
      "metadata": {
        "tags": [
          "output_scroll"
        ]
      },
      "source": [
        "from architxt.schema import Schema\n",
        "\n",
        "schema = Schema.from_forest(forest_copy, keep_unlabelled=False)\n",
        "print(schema.as_cfg())"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "2ba1cc1cae9bcbf7",
      "metadata": {},
      "source": [
        "We've successfully built a basic database schema from our corpus, but there's significant potential for improvement.\n",
        "Let's explore how we can enhance it using the **ArchiTXT** simplification algorithm!\n",
        "\n",
        "First, let's visualize the repartition of equivalent classes inside the forest."
      ]
    },
    {
      "metadata": {
        "tags": [
          "remove-output"
        ]
      },
      "cell_type": "code",
      "source": [
        "from architxt.similarity import equiv_cluster\n",
        "\n",
        "clusters = equiv_cluster(forest, tau=0.8)"
      ],
      "id": "356fb8907ad1db4f",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "tags": [
          "hide-input"
        ]
      },
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "\n",
        "fig = px.bar(sum(tree.height == 2 for tree in klass) for klass in clusters)\n",
        "fig.update_layout(xaxis_title='Equivalent Class', yaxis_title='Count', showlegend=False)"
      ],
      "id": "47ecb14c62535503",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "It's now time to use **ArchiTXT** to automatically structure the data.",
      "id": "54bb0f1c2802fb9d"
    },
    {
      "cell_type": "code",
      "id": "130878755df87067",
      "metadata": {
        "tags": [
          "remove-output"
        ]
      },
      "source": [
        "from architxt.simplification.tree_rewriting import rewrite\n",
        "\n",
        "rewrite(forest, epoch=10, min_support=5, tau=0.8)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "e1858be3938f3877",
      "metadata": {},
      "source": [
        "# Look at the highest tree\n",
        "max(forest, key=lambda tree: tree.height).pretty_print()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "4416adaa8b722e2f",
      "metadata": {},
      "source": "We now have a more granular structure. Let's take a closer look at the schema."
    },
    {
      "cell_type": "code",
      "id": "d5a443ba85dbeb0",
      "metadata": {},
      "source": [
        "schema = Schema.from_forest(forest, keep_unlabelled=False)\n",
        "print(schema.as_cfg())"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "b87a797643c215d8",
      "metadata": {},
      "source": [
        "The schema is now much smaller, and the groups are more meaningful.\n",
        "\n",
        "But not all extracted trees provide valuable insights, so we could filter the structured instance to keep only the **valid trees** using `schema.extract_valid_trees(new_forest)`.\n",
        "Let's explore the different **semantic groups**.\n",
        "Groups represent common patterns across the corpus."
      ]
    },
    {
      "cell_type": "code",
      "id": "fd5cbaae5c677d08",
      "metadata": {},
      "source": [
        "all_datasets = schema.extract_datasets(forest)\n",
        "group, dataset = max(all_datasets.items(), key=lambda x: len(x[1]))\n",
        "\n",
        "print(f'Group: {group.name}')\n",
        "\n",
        "dataset"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
